# Import functions for data reading and cleaning
import dataPreprocess as dp

# Import sklearn functions for LDA model
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer

# Imports to visualize the LDA model
import pyLDAvis
import pyLDAvis.sklearn
import numpy as np
import pandas as pd

"""
    Create doc-word matrix from given dataset using a CountVectorizer
    @params:
        dataset_csv (string): name of training dataset csv file, file must have column named "description"
        minimum_df (int): words that occur atleast minimum_df times is considered for the model
    @return:
        data_vectorized: doc-word matrix
        vectorizer: CountVectorizer class instance used to create doc-word matrix
"""
def get_data_vectorized(dataset_csv, minimum_df):
    #Initialize CountVectorizer with configurations to only consider words that occur atleast given minimum_df times
    vectorizer = CountVectorizer(analyzer='word',
                             min_df=minimum_df,
                             stop_words='english',             # remove stop words
                             lowercase=True,                   # convert all words to lowercase
                             token_pattern='[a-zA-Z0-9]{3,}',  # words should be atleast 3 chars long
                             )
    #Read the provided dataset
    descriptions_data=dp.read_data(dataset_csv)
    #Tokenize, remove stopwords, case-fold, lemmatize the data
    dp.preprocess(descriptions_data)
    #Create doc-word matrix which is the input for the LDA model
    data_vectorized=vectorizer.fit_transform(descriptions_data)
    return data_vectorized,vectorizer

"""
    Creates lda_model with given number_of_topics and doc-word matrix from dataset
    @params:
        data_vectorized: doc-word matrix created from the dataset
        num_of_topics (int): number of topics that are generated by the lda model
    @return:
        lda_model (LatentDirichletAllocation): lda model
"""
def create_lda_model(data_vectorized,num_of_topics):
    #Initialize an LDA model with configurations
    lda_model = LatentDirichletAllocation(n_components=num_of_topics,         # Set number of topics to given number
                                        max_iter=10,                          # Max learning iterations
                                        learning_method='online',             # Online method is faster for large datasets
                                        random_state=100,                     # Random state to reproduce same topics
                                        batch_size=128,                       # Documents in each learning iteration
                                        evaluate_every = -1,                  # compute perplexity every n iters, default: Don't
                                        n_jobs = -1,                          # Use all available CPUs
                                        learning_decay=.6                     # Best learning decay based on log likelihood scored
                                        )
    #Create an LDA model with the given dataset
    lda_model.fit_transform(data_vectorized)
    #Return the LDA model, vectorizer and the doc-word matrix
    return lda_model

"""
    Create "model.html" to show the topics created by the given LDA model
    @params:
        lda_model (LatentDirichletAllocation): lda model
        data_vectorized: doc-word matrix
        vectorizer: CountVectorizer class instance used to create doc-word matrix
    @output:
        model.html (file): model visualization
"""
def visualize_lda_model(lda_model,data_vectorized,vectorizer):
    #Prepare the visualization
    visualization = pyLDAvis.sklearn.prepare(lda_model, data_vectorized, vectorizer, mds='tsne')
    #Save the visualization as 'model.html'
    try:
        pyLDAvis.save_html(visualization,'model.html')
    #Log error in visualization creation
    except:
        print("Failed to create visualization")

"""
    Returns the perplexity score of the provided LDA model
    @params:
        lda_model (LatentDirichletAllocation): lda model
        data_vectorized: doc-word matrix
    @returns:
        perplexity(float): the perplexity of the provided lda model on provided dataset
"""
def get_perplexity(lda_model,data_vectorized):
    lda_model.perplexity(data_vectorized)

"""
    Returns and saves a dataframe with top specified number words for each topic in the given LDA model and vectorizer
    @param:
        lda_model (LatentDirichletAllocation): lda model
        vectorizer: CountVectorizer class instance used to create doc-word matrix
        num(int): number of top words in dataframe for each topic
    @return:
        topic_keywords(pandas dataframe): dataframe is also saved as 'topic_words.csv' 
"""
def show_topics(vectorizer, lda_model, num):
    #  Np array of the feature names in passed vectorizer
    keywords = np.array(vectorizer.get_feature_names())
    topic_keywords = []
    # extract the top words of each topic from the given LDA model
    for topic_weights in lda_model.components_:
        # Sort and get top words based on parameter number
        top_keyword_locs = (-topic_weights).argsort()[:num]
        # Add topwords to list
        topic_keywords.append(keywords.take(top_keyword_locs))
    # Save the topic distribution as a pandas csv
    df_topic_keywords = pd.DataFrame(topic_keywords)
    # Word number
    df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]
    # Topic number
    df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]
    # Save the dataframe
    df_topic_keywords.to_csv("topic_words.csv",index=False)
    return df_topic_keywords