"""
    Author: Sadab Hafiz
    Description: This file contains functions to create,visualize and use an LDA model.
"""
# Import for saving topics info
import numpy as np
import pandas as pd
# Imports to visualize the LDA model
import pyLDAvis
import pyLDAvis.sklearn
# Import sklearn functions for LDA model
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer

# Import functions for data reading and cleaning
import dataPreprocess as dp


def get_data_vectorized(dataset_csv, minimum_df, maximum_df):
    """
    Create doc-word matrix from given dataset using a CountVectorizer
    @params:
        dataset_csv (string): name of training dataset csv file, file must have column named "description"
        minimum_df (int): words that occur atleast minimum_df times is considered for the model
        maximum_df (float): words that occur more than this frequency is ignored
    @return:
        data_vectorized: doc-word matrix
        vectorizer: CountVectorizer class instance used to create doc-word matrix
    """
    # Initialize CountVectorizer with configurations to only consider words that occur atleast given minimum_df times
    vectorizer = CountVectorizer(
        analyzer="word",
        min_df=minimum_df,
        max_df=maximum_df,  # Ignore words more than this frequency
        stop_words="english",  # remove stop words
        lowercase=True,  # convert all words to lowercase
        token_pattern="[a-zA-Z]{3,}",  # words should be atleast 3 chars long
    )
    # Read the provided dataset
    descriptions_data = dp.read_data(dataset_csv)
    # Tokenize, remove stopwords, case-fold, lemmatize the data
    dp.preprocess(descriptions_data)
    # Create doc-word matrix which is the input for the LDA model
    data_vectorized = vectorizer.fit_transform(descriptions_data)
    return data_vectorized, vectorizer


def create_lda_model(data_vectorized, num_of_topics):
    """
    Creates lda_model with given number_of_topics and doc-word matrix from dataset
    @params:
        data_vectorized: doc-word matrix created from the dataset
        num_of_topics (int): number of topics that are generated by the lda model
    @return:
        lda_model (LatentDirichletAllocation): lda model
    """
    # Initialize an LDA model with configurations
    lda_model = LatentDirichletAllocation(
        n_components=num_of_topics,  # Set number of topics to given number
        max_iter=10,  # Max learning iterations
        learning_method="online",  # Online method is faster for large datasets
        random_state=100,  # Random state to reproduce same topics
        batch_size=128,  # Documents in each learning iteration
        evaluate_every=-1,  # compute perplexity every n iters, default: Don't
        n_jobs=-1,  # Use all available CPUs
        learning_decay=0.6,  # Best learning decay based on log likelihood scored
    )
    # Create an LDA model with the given dataset
    lda_model.fit_transform(data_vectorized)
    # Return the LDA model, vectorizer and the doc-word matrix
    return lda_model


def visualize_lda_model(lda_model, data_vectorized, vectorizer):
    """
    Create "model.html" to show the topics created by the given LDA model
    @params:
        lda_model (LatentDirichletAllocation): lda model
        data_vectorized: doc-word matrix
        vectorizer: CountVectorizer class instance used to create doc-word matrix
    @output:
        model.html (file): model visualization
    """
    # Prepare the visualization
    visualization = pyLDAvis.sklearn.prepare(
        lda_model, data_vectorized, vectorizer, mds="tsne"
    )
    # Save the visualization as 'model.html'
    try:
        pyLDAvis.save_html(visualization, "model.html")
    # Log error in visualization creation
    except Exception:
        print("Failed to create visualization")


def get_perplexity(lda_model, data_vectorized):
    """
    Returns the perplexity score of the provided LDA model
    @params:
        lda_model (LatentDirichletAllocation): lda model
        data_vectorized: doc-word matrix
    @returns:
        perplexity(float): the perplexity of the provided lda model on provided dataset
    """
    lda_model.perplexity(data_vectorized)


def show_topics(vectorizer, lda_model, num):
    """
    Returns and saves a dataframe with top specified number words for each topic in the given LDA model and vectorizer
    @param:
        lda_model (LatentDirichletAllocation): lda model
        vectorizer: CountVectorizer class instance used to create doc-word matrix
        num(int): number of top words in dataframe for each topic
    @return:
        topic_keywords(pandas dataframe): dataframe is also saved as 'topic_words.csv'
    """
    #  Np array of the feature names in passed vectorizer
    keywords = np.array(vectorizer.get_feature_names_out())
    topic_keywords = []
    # extract the top words of each topic from the given LDA model
    for topic_weights in lda_model.components_:
        # Sort and get top words based on parameter number
        top_keyword_locs = (-topic_weights).argsort()[:num]
        # Add topwords to list
        topic_keywords.append(keywords.take(top_keyword_locs))
    # Save the topic distribution as a pandas csv
    df_topic_keywords = pd.DataFrame(topic_keywords)
    # Word number
    df_topic_keywords.columns = [
        "Word " + str(i) for i in range(df_topic_keywords.shape[1])
    ]
    # Topic number
    df_topic_keywords.index = [
        "Topic " + str(i) for i in range(1, df_topic_keywords.shape[0]+1)
    ]
    # Save the dataframe
    df_topic_keywords.to_csv("topic_words.csv")
    return df_topic_keywords


def predict(query_description, vectorizer, lda_model, df_topic_keywords):
    """
    Predits the topic for the given string using the passed lda_model and returns topic keywords and topic probabilities for each topic
    @params:
        query_description (string): query description
        lda_model (LatentDirichletAllocation): lda model
        vectorizer: CountVectorizer class instance used to create doc-word matrix
        df_topic_keywords (pandas dataframe): dataframe with top words for each topic
    @return:
        topic_distribution (2d list[list of strings, float]): Contains keywords for each topic and the frequency of the topic in query description
    """
    # Return empty lists if the input is invalid
    if not query_description:
        print("invalid input")
        return [], []
    # Clean the query_description and prepare it for CountVectorizer
    dp.preprocess_single(query_description)
    query_description = [query_description]
    # Vectorize the query_description to prepare it for LDA model
    query_description = vectorizer.transform(query_description)

    # Check the topic of the query_description using the passed LDA model
    topic_probability_scores = lda_model.transform(query_description)
    # Get the top words for the each topics
    topics_list = df_topic_keywords.values.tolist()
    # Append the topic keywords and probability of all topics into an array
    topic_distribution = []
    for index, topic in enumerate(topics_list):
        topic_distribution.append(topic, topic_probability_scores[0][index])
    # Sort the topics based on the frequency
    # topic_distribution.sort(key=lambda row:(row[1]), reverse=True)
    return topic_distribution
