# Import functions for data reading and cleaning
import dataPreprocess as dp

# Import sklearn functions for LDA model
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer

# Imports to visualize the LDA model
import pyLDAvis
import pyLDAvis.sklearn
import numpy as np
import pandas as pd

"""
    Create doc-word matrix from given dataset using a CountVectorizer
    @params:
        dataset_csv (string): name of training dataset csv file, file must have column named "description"
        minimum_df (int): words that occur atleast minimum_df times is considered for the model
    @return:
        data_vectorized: doc-word matrix
        vectorizer: CountVectorizer class instance used to create doc-word matrix
"""
def get_data_vectorized(dataset_csv, minimum_df):
    #Initialize CountVectorizer with configurations to only consider words that occur atleast given minimum_df times
    vectorizer = CountVectorizer(analyzer='word',
                             min_df=minimum_df,
                             stop_words='english',             # remove stop words
                             lowercase=True,                   # convert all words to lowercase
                             token_pattern='[a-zA-Z0-9]{3,}',  # words should be atleast 3 chars long
                             )
    #Read the provided dataset
    descriptions_data=dp.read_data(dataset_csv)
    #Tokenize, remove stopwords, case-fold, lemmatize the data
    dp.preprocess(descriptions_data)
    #Create doc-word matrix which is the input for the LDA model
    data_vectorized=vectorizer.fit_transform(descriptions_data)
    return data_vectorized,vectorizer

"""
    Creates lda_model with given number_of_topics and doc-word matrix from dataset
    @params:
        data_vectorized: doc-word matrix created from the dataset
        num_of_topics (int): number of topics that are generated by the lda model
    @return:
        lda_model (LatentDirichletAllocation): lda model
"""
def create_lda_model(data_vectorized,num_of_topics):
    #Initialize an LDA model with configurations
    lda_model = LatentDirichletAllocation(n_components=num_of_topics,         # Set number of topics to given number
                                        max_iter=10,                          # Max learning iterations
                                        learning_method='online',             # Online method is faster for large datasets
                                        random_state=100,                     # Random state to reproduce same topics
                                        batch_size=128,                       # Documents in each learning iteration
                                        evaluate_every = -1,                  # compute perplexity every n iters, default: Don't
                                        n_jobs = -1,                          # Use all available CPUs
                                        learning_decay=.6                     # Best learning decay based on log likelihood scored
                                        )
    #Create an LDA model with the given dataset
    lda_model.fit_transform(data_vectorized)
    #Return the LDA model, vectorizer and the doc-word matrix
    return lda_model

"""
    Create "model.html" to show the topics created by the given LDA model
    @params:
        lda_model (LatentDirichletAllocation): lda model
        data_vectorized: doc-word matrix
        vectorizer: CountVectorizer class instance used to create doc-word matrix
    @output:
        model.html (file): model visualization
"""
def visualize_lda_model(lda_model,data_vectorized,vectorizer):
    #Prepare the visualization
    visualization = pyLDAvis.sklearn.prepare(lda_model, data_vectorized, vectorizer, mds='tsne')
    #Save the visualization as 'model.html'
    try:
        pyLDAvis.save_html(visualization,'model.html')
    #Log error in visualization creation
    except:
        print("Failed to create visualization")

"""
    Returns the perplexity score of the provided LDA model
    @params:
        lda_model (LatentDirichletAllocation): lda model
        data_vectorized: doc-word matrix
    @returns:
        perplexity(float): the perplexity of the provided lda model on provided dataset
"""
def get_perplexity(lda_model,data_vectorized):
    lda_model.perplexity(data_vectorized)

"""
    Returns and saves a dataframe with top specified number words for each topic in the given LDA model and vectorizer
    @param:
        lda_model (LatentDirichletAllocation): lda model
        vectorizer: CountVectorizer class instance used to create doc-word matrix
        num(int): number of top words in dataframe for each topic
    @return:
        topic_keywords(pandas dataframe): dataframe is also saved as 'topic_words.csv' 
"""
def show_topics(vectorizer, lda_model, num):
    #  Np array of the feature names in passed vectorizer
    keywords = np.array(vectorizer.get_feature_names())
    topic_keywords = []
    # extract the top words of each topic from the given LDA model
    for topic_weights in lda_model.components_:
        # Sort and get top words based on parameter number
        top_keyword_locs = (-topic_weights).argsort()[:num]
        # Add topwords to list
        topic_keywords.append(keywords.take(top_keyword_locs))
    # Save the topic distribution as a pandas csv
    df_topic_keywords = pd.DataFrame(topic_keywords)
    # Word number
    df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]
    # Topic number
    df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]
    # Save the dataframe
    df_topic_keywords.to_csv("topic_words.csv",index=False)
    return df_topic_keywords

"""
    Predits the topic for the given string using the passed lda_model and returns top keywords and topic probability
    @params:
        query_description (string): query description
        lda_model (LatentDirichletAllocation): lda model
        vectorizer: CountVectorizer class instance used to create doc-word matrix
        df_topic_keywords (pandas dataframe): dataframe with top words for each topic
    @return:
        topic (list of strings): top words for the predicted topic
        topic_probability_scores (list of floats): the prevalance of all topics on the query_description
"""
def predict(query_description, vectorizer,lda_model,df_topic_keywords):
    # Return empty lists if the input is invalid
    if not query_description:
        print("invalid input")
        return [],[]
    # Clean the query_description and prepare it for CountVectorizer
    dp.preprocess_single(query_description)
    query_description=[query_description]
    # Vectorize the query_description to prepare it for LDA model
    query_description= vectorizer.transform(query_description)

    # Check the topic of the query_description using the passed LDA model
    topic_probability_scores = lda_model.transform(query_description)
    # Get the top words for the topic with highest probability
    topic = df_topic_keywords.iloc[np.argmax(topic_probability_scores), :].values.tolist()
    return topic, topic_probability_scores

"""
    Creating an LDA model with best parameters predicted by grid search
"""
data_vectorized,vectorizer=get_data_vectorized('books.csv',10)
lda_model=create_lda_model(data_vectorized,10)
df_topic_keywords = show_topics(vectorizer, lda_model,15)
"""
    Example code showing how to predict the probability of given text using the LDA model
"""
predicted_topic,probability_scores=predict("In The Problem of Pain, C.S. Lewis, one of the most renowned Christian authors and thinkers,examines a universally applicable question within the human condition: If God is good and all-powerful, why does he allow his creatures to suffer pain? With his signature wealth of compassion and insight, C.S. Lewis offers answers to these crucial questions and shares his hope and wisdom to help heal a world hungering for a true understanding of human nature",vectorizer,lda_model,df_topic_keywords)
print(predicted_topic)
print(probability_scores)